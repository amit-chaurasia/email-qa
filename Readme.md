# üìß Email Extractor API (FastAPI + Ollama + Mistral)

This project provides a FastAPI-based web service that extracts the latest composed email from a raw thread using `email-reply-parser` and locally hosted open-source LLMs (via Ollama). It's ideal for pulling only the most recent human-composed message from multi-reply email chains.

---

## üöÄ Features

-   Accepts raw email input via plain text (`text/plain`)
-   Uses `email-reply-parser` to isolate the most recent composed message
-   Sends cleaned prompt to an LLM (e.g. Mistral) running locally via Ollama
-   Returns clean body content with greeting, message, and sign-off

---

## üß± Project Structure

```
email-extractor/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ main.py               # FastAPI app logic
‚îú‚îÄ‚îÄ .env                      # Environment variables (OLLAMA_HOST, OLLAMA_MODEL)
‚îú‚îÄ‚îÄ Dockerfile                # Docker build for FastAPI service
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ docker-compose.yml        # Runs Ollama + API service
```

---

## üõ† Makefile Commands

The following commands simplify Docker and API interaction:

| Command           | Description                             |
| ----------------- | --------------------------------------- |
| `make up`         | Start all containers in the background  |
| `make pull-model` | Pull the Mistral model into Ollama      |
| `make rebuild`    | Rebuild containers and restart services |
| `make down`       | Stop all services                       |
| `make logs`       | View real-time logs                     |
| `make curl`       | Send a sample email from `email.txt`    |

> ‚úÖ Recommended first-time setup:
>
> ```bash
> make up
> make pull-model
> ```

---

## ‚öôÔ∏è Setup Instructions

### 1. Clone & prepare

```bash
git clone <repo>
cd email-extractor
```

### 2. Create `.env` file

```env
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=mistral
```

### 3. Build & start services

```bash
docker compose up --build -d
```

### 4. Pull model into Ollama (first-time only)

```bash
docker exec -it ollama ollama pull mistral
```

---

## üß™ How to Use

### ‚ñ∂Ô∏è Plain text input

```bash
curl -X POST http://localhost:8000/extract \
     -H "Content-Type: text/plain" \
     --data-binary @email.txt
```

---

## üß∞ Troubleshooting

### ‚ùå API returns `model not found`

You forgot to pull the model:

```bash
docker exec -it ollama ollama pull mistral
```

### ‚ùå API returns `Failed to connect to Ollama`

Ensure:

-   `.env` or Docker `environment:` sets `OLLAMA_HOST=http://ollama:11434`
-   `ollama` container is running
-   You‚Äôre using Docker Compose, not `localhost` from within the `api` container

### ‚úÖ Verify Ollama is reachable

```bash
docker compose exec api curl http://ollama:11434/api/tags
```

Should return model list.

---

## üìú License

MIT

---

Let me know if you want to:

-   Add a `Makefile`
-   Bundle `.http` files for REST client testing
-   Extend to support summarization or intent detection
